{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant as a demonstration to show how exactly LLMs and Transformers work and why they are so effective. We will start off assuming that you have some basic knowledge about how neural networks work in PyTorch. Below we will create the model architecture using some prebuilt layers from the PyTorch package. After that we will demonstrate how exactly those layers are constructed. Then we will train the model and demonstrate that it learned something.\n",
    "\n",
    "For the below model instead of using words we will just use numbers to try to predict the next number in a sequence of numbers. Under the hood this is what LLMs do as they map each word to a series of numbers because the models can only understand numbers and not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallLLM(\n",
       "  (embedding): Embedding(10, 32)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SmallLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
    "        super(SmallLLM, self).__init__()\n",
    "        \n",
    "        # This is an interesting embedding layer we will be digging deeper into later in this tutorial\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # This layer is also likely new and is the fundamental strength of the new series of LLMs coming out\n",
    "        # The transformer architecture beats the previously dominant RNNs and other NLP models\n",
    "        self.transformer = nn.Transformer(embed_size, num_heads, num_layers)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x, x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Since we are working with 10 numbers (0-9) our vocab size will be 10\n",
    "vocab_size = 10  \n",
    "\n",
    "# You can treat these as hyper-parameters of our model\n",
    "embed_size = 32  \n",
    "num_heads = 2    \n",
    "num_layers = 2 \n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SmallLLM(vocab_size, embed_size, num_heads, num_layers)\n",
    "\n",
    "# We use CrossEntropyLoss because we are in essence looking to classify the next token in the series\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Show the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dig into what both of those layers are really doing. First we can look into the embedding layer. Please note that the embdedding layer expects an integer input and if you are curious how we get from words -> integers as our model inputs then please look at the tokenization notebook.\n",
    "\n",
    "Below we define an embedding layer. Essentially this layer maps integeres (0-vocab_size) and produces a embedding size dimensional output, in this case 3. The idea here is that our model will be able to learn the relationship between different integers and a multi-dimensional continuous output which feeds into the transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9070,  0.9123, -0.1542],\n",
       "        [-1.0586,  0.0818, -1.3950],\n",
       "        [-1.2679, -1.3518, -1.0793],\n",
       "        [-0.8483,  1.1451, -0.5357]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here we define an embedding layer with a vocab size of 5 and an dimensions of embeddings of 3\n",
    "embedding = nn.Embedding(5, 3)\n",
    "\n",
    "# This will be the input to our embedding layer, feel free to mess around with it\n",
    "input_indices = torch.LongTensor([1, 2, 4, 0])\n",
    "\n",
    "# Pass in our input vocab\n",
    "embedded = embedding(input_indices)\n",
    "\n",
    "# Display our result which is essentially random at this point\n",
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the purpose of the embedding layer is to turn our vocabulary integers into a multidimensional continuous output. This helps us extract the maximum amount of information from the vocabulary inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look into the transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it all together and train our numerical sequence predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0119\n",
      "Epoch [2/50], Loss: 0.0109\n",
      "Epoch [3/50], Loss: 0.0103\n",
      "Epoch [4/50], Loss: 0.0096\n",
      "Epoch [5/50], Loss: 0.0089\n",
      "Epoch [6/50], Loss: 0.0083\n",
      "Epoch [7/50], Loss: 0.0080\n",
      "Epoch [8/50], Loss: 0.0074\n",
      "Epoch [9/50], Loss: 0.0068\n",
      "Epoch [10/50], Loss: 0.0065\n",
      "Epoch [11/50], Loss: 0.0063\n",
      "Epoch [12/50], Loss: 0.0059\n",
      "Epoch [13/50], Loss: 0.0056\n",
      "Epoch [14/50], Loss: 0.0052\n",
      "Epoch [15/50], Loss: 0.0051\n",
      "Epoch [16/50], Loss: 0.0047\n",
      "Epoch [17/50], Loss: 0.0045\n",
      "Epoch [18/50], Loss: 0.0043\n",
      "Epoch [19/50], Loss: 0.0042\n",
      "Epoch [20/50], Loss: 0.0039\n",
      "Epoch [21/50], Loss: 0.0039\n",
      "Epoch [22/50], Loss: 0.0038\n",
      "Epoch [23/50], Loss: 0.0035\n",
      "Epoch [24/50], Loss: 0.0034\n",
      "Epoch [25/50], Loss: 0.0033\n",
      "Epoch [26/50], Loss: 0.0031\n",
      "Epoch [27/50], Loss: 0.0030\n",
      "Epoch [28/50], Loss: 0.0029\n",
      "Epoch [29/50], Loss: 0.0028\n",
      "Epoch [30/50], Loss: 0.0027\n",
      "Epoch [31/50], Loss: 0.0026\n",
      "Epoch [32/50], Loss: 0.0025\n",
      "Epoch [33/50], Loss: 0.0025\n",
      "Epoch [34/50], Loss: 0.0024\n",
      "Epoch [35/50], Loss: 0.0023\n",
      "Epoch [36/50], Loss: 0.0021\n",
      "Epoch [37/50], Loss: 0.0021\n",
      "Epoch [38/50], Loss: 0.0021\n",
      "Epoch [39/50], Loss: 0.0019\n",
      "Epoch [40/50], Loss: 0.0019\n",
      "Epoch [41/50], Loss: 0.0019\n",
      "Epoch [42/50], Loss: 0.0018\n",
      "Epoch [43/50], Loss: 0.0017\n",
      "Epoch [44/50], Loss: 0.0017\n",
      "Epoch [45/50], Loss: 0.0017\n",
      "Epoch [46/50], Loss: 0.0016\n",
      "Epoch [47/50], Loss: 0.0016\n",
      "Epoch [48/50], Loss: 0.0016\n",
      "Epoch [49/50], Loss: 0.0015\n",
      "Epoch [50/50], Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some dummy data\n",
    "# Sequences are arrays from 0 to 9 (our vocab)\n",
    "# For each sequence, the target is the sequence shifted by one position\n",
    "sequences = np.array([[i for i in range(10)] for _ in range(100)])\n",
    "targets = np.array([[i for i in range(1, 10)] + [0] for _ in range(100)])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "sequences = torch.LongTensor(sequences)\n",
    "targets = torch.LongTensor(targets)\n",
    "\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(sequences, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_seq, batch_target in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_seq)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        outputs = outputs.view(-1, vocab_size)\n",
    "        batch_target = batch_target.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our trained model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing the generate_numbers function to have correct input dimensions\n",
    "def generate_numbers(model, start_sequence, length):\n",
    "    generated_sequence = start_sequence.copy()\n",
    "    \n",
    "    for _ in range(length):\n",
    "        input_sequence = torch.LongTensor([generated_sequence[-10:]]).transpose(0, 1)\n",
    "        output = model(input_sequence)\n",
    "        next_number = torch.argmax(output[0, -1]).item()\n",
    "        generated_sequence.append(next_number)\n",
    "    \n",
    "    return generated_sequence\n",
    "\n",
    "# Generate a sequence starting with [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "generated_sequence = generate_numbers(model, [i for i in range(10)], 20)\n",
    "generated_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just as a short example we can switch from a numerical sequence predictor to a word based model and see what the results are. This example will encorporate a lot of subjects detailed in this notebook and in the tokenization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/zfara/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.871013979905776, 7.673558832058651)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('state_union')\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "# Fetch sentences from the reuters dataset, we use the state of the union dataset\n",
    "# Maybe our LLM will be speaking like a politician\n",
    "sentences = [' '.join(sent) for sent in state_union.sents()[:1000]]\n",
    "\n",
    "\n",
    "# Tokenize and build vocabulary\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "flat_tokens = [token for sublist in tokenized_sentences for token in sublist]\n",
    "vocab_counter = Counter(flat_tokens)\n",
    "\n",
    "# Build the integer mapping between the words and their corresponding integer\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(vocab_counter.most_common())}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "index_to_word = {i: word for word, i in vocab.items()}\n",
    "\n",
    "# Encode with our new mapping between words and integers\n",
    "encoded_sentences = [[vocab.get(token, vocab[\"<UNK>\"]) for token in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "# Generate sequences and targets which we will later make into training and testing sets\n",
    "sequences = []\n",
    "targets = []\n",
    "sequence_length = 5 # Change this so that we will be able to make more complex sentences\n",
    "\n",
    "for sentence in encoded_sentences:\n",
    "    for i in range(len(sentence) - sequence_length + 1):\n",
    "        sequences.append(sentence[i:i + sequence_length - 1])\n",
    "        targets.append(sentence[i + 1:i + sequence_length])\n",
    "\n",
    "# Training and validation split\n",
    "split_idx = int(0.8 * len(sequences))\n",
    "train_sequences, val_sequences = sequences[:split_idx], sequences[split_idx:]\n",
    "train_targets, val_targets = targets[:split_idx], targets[split_idx:]\n",
    "\n",
    "# Convert to PyTorch tensors which we will make dataloaders and then use in our training loop\n",
    "train_sequences = torch.LongTensor(train_sequences)\n",
    "train_targets = torch.LongTensor(train_targets)\n",
    "val_sequences = torch.LongTensor(val_sequences)\n",
    "val_targets = torch.LongTensor(val_targets)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(train_sequences, train_targets)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_sequences, val_targets)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Initialize model, criterion and the optimzer\n",
    "model = SmallLLM(vocab_size, embed_size, num_heads, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "epochs = 25  # This will determine how long the model trains for\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_seq, batch_target in train_dataloader:\n",
    "        outputs = model(batch_seq)\n",
    "        outputs = outputs.view(-1, vocab_size)\n",
    "        batch_target = batch_target.view(-1)\n",
    "        loss = criterion(outputs, batch_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_seq, batch_target in val_dataloader:\n",
    "            outputs = model(batch_seq)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            batch_target = batch_target.view(-1)\n",
    "            loss = criterion(outputs, batch_target)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "\n",
    "final_train_loss, final_val_loss # Print our final metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look below we didn't really improve on our validation set. This is probably because we used a super small model, a short training time and a relatively complex corpus. However, we still should be able to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x296d9f940>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can be been the congress\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, initial_sequence, length, vocab, index_to_word):\n",
    "    model.eval()\n",
    "    generated_sequence = initial_sequence.copy()\n",
    "    \n",
    "    for _ in range(length):\n",
    "        input_sequence = torch.LongTensor([generated_sequence[-5:]]).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        with torch.no_grad():\n",
    "            output = model(input_sequence)\n",
    "        next_word_idx = torch.argmax(output[0, -1]).item()\n",
    "        \n",
    "        generated_sequence.append(next_word_idx)\n",
    "    \n",
    "    generated_sentence = ' '.join([index_to_word[idx] for idx in generated_sequence])\n",
    "    return generated_sentence\n",
    "\n",
    "# Example usage\n",
    "initial_sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in [\"how\",\"can\"]]\n",
    "generated_sentence = generate_text(model, initial_sequence, 4, vocab, index_to_word)\n",
    "print(generated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
