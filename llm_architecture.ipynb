{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant as a demonstration to show how exactly LLMs and Transformers work and why they are so effective. We will start off assuming that you have some basic knowledge about how neural networks work in PyTorch. Below we will create the model architecture using some prebuilt layers from the PyTorch package. After that we will demonstrate how exactly those layers are constructed. Then we will train the model and demonstrate that it learned something.\n",
    "\n",
    "For the below model instead of using words we will just use numbers to try to predict the next number in a sequence of numbers. Under the hood this is what LLMs do as they map each word to a series of numbers because the models can only understand numbers and not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallLLM(\n",
       "  (embedding): Embedding(10, 32)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SmallLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
    "        super(SmallLLM, self).__init__()\n",
    "        \n",
    "        # This is an interesting embedding layer we will be digging deeper into later in this tutorial\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # This layer is also likely new and is the fundamental strength of the new series of LLMs coming out\n",
    "        # The transformer architecture beats the previously dominant RNNs and other NLP models\n",
    "        self.transformer = nn.Transformer(embed_size, num_heads, num_layers)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x, x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Since we are working with 10 numbers (0-9) our vocab size will be 10\n",
    "vocab_size = 10  \n",
    "\n",
    "# You can treat these as hyper-parameters of our model\n",
    "embed_size = 32  \n",
    "num_heads = 2    \n",
    "num_layers = 2 \n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SmallLLM(vocab_size, embed_size, num_heads, num_layers)\n",
    "\n",
    "# We use CrossEntropyLoss because we are in essence looking to classify the next token in the series\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Show the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dig into what both of those layers are really doing. First we can look into the embedding layer. Please note that the embdedding layer expects an integer input and if you are curious how we get from words -> integers as our model inputs then please look at the tokenization notebook.\n",
    "\n",
    "Below we define an embedding layer. Essentially this layer maps integeres (0-vocab_size) and produces a embedding size dimensional output, in this case 3. The idea here is that our model will be able to learn the relationship between different integers and a multi-dimensional continuous output which feeds into the transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9070,  0.9123, -0.1542],\n",
       "        [-1.0586,  0.0818, -1.3950],\n",
       "        [-1.2679, -1.3518, -1.0793],\n",
       "        [-0.8483,  1.1451, -0.5357]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here we define an embedding layer with a vocab size of 5 and an dimensions of embeddings of 3\n",
    "embedding = nn.Embedding(5, 3)\n",
    "\n",
    "# This will be the input to our embedding layer, feel free to mess around with it\n",
    "input_indices = torch.LongTensor([1, 2, 4, 0])\n",
    "\n",
    "# Pass in our input vocab\n",
    "embedded = embedding(input_indices)\n",
    "\n",
    "# Display our result which is essentially random at this point\n",
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
